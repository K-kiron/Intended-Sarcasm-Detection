{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d74e66d1-1ab6-4637-a7bd-a8136d483a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240f91c5-11a4-45c0-8d4f-bf1366ce8aed",
   "metadata": {},
   "source": [
    "# Overview of english training data(train.En.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8801b572-825b-4629-b7c8-d48c2003edfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_en = pd.read_csv('train/train.En.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80a74795-b03a-4ab2-ac46-ace4debbbe0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "      <th>rephrase</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>irony</th>\n",
       "      <th>satire</th>\n",
       "      <th>understatement</th>\n",
       "      <th>overstatement</th>\n",
       "      <th>rhetorical_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The only thing I got from college is a caffein...</td>\n",
       "      <td>1</td>\n",
       "      <td>College is really difficult, expensive, tiring...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I love it when professors draw a big question ...</td>\n",
       "      <td>1</td>\n",
       "      <td>I do not like when professors don’t write out ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Remember the hundred emails from companies whe...</td>\n",
       "      <td>1</td>\n",
       "      <td>I, at the bare minimum, wish companies actuall...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Today my pop-pop told me I was not “forced” to...</td>\n",
       "      <td>1</td>\n",
       "      <td>Today my pop-pop told me I was not \"forced\" to...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
       "      <td>1</td>\n",
       "      <td>I would say Ted Cruz is an asshole and doesn’t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3463</th>\n",
       "      <td>3463</td>\n",
       "      <td>The population spike in Chicago in 9 months is...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>3464</td>\n",
       "      <td>You'd think in the second to last English clas...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>3465</td>\n",
       "      <td>I’m finally surfacing after a holiday to Scotl...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>3466</td>\n",
       "      <td>Couldn't be prouder today. Well done to every ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3467</th>\n",
       "      <td>3467</td>\n",
       "      <td>Overheard as my 13 year old games with a frien...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3468 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              tweet  \\\n",
       "0              0  The only thing I got from college is a caffein...   \n",
       "1              1  I love it when professors draw a big question ...   \n",
       "2              2  Remember the hundred emails from companies whe...   \n",
       "3              3  Today my pop-pop told me I was not “forced” to...   \n",
       "4              4  @VolphanCarol @littlewhitty @mysticalmanatee I...   \n",
       "...          ...                                                ...   \n",
       "3463        3463  The population spike in Chicago in 9 months is...   \n",
       "3464        3464  You'd think in the second to last English clas...   \n",
       "3465        3465  I’m finally surfacing after a holiday to Scotl...   \n",
       "3466        3466  Couldn't be prouder today. Well done to every ...   \n",
       "3467        3467  Overheard as my 13 year old games with a frien...   \n",
       "\n",
       "      sarcastic                                           rephrase  sarcasm  \\\n",
       "0             1  College is really difficult, expensive, tiring...      0.0   \n",
       "1             1  I do not like when professors don’t write out ...      1.0   \n",
       "2             1  I, at the bare minimum, wish companies actuall...      0.0   \n",
       "3             1  Today my pop-pop told me I was not \"forced\" to...      1.0   \n",
       "4             1  I would say Ted Cruz is an asshole and doesn’t...      1.0   \n",
       "...         ...                                                ...      ...   \n",
       "3463          0                                                NaN      NaN   \n",
       "3464          0                                                NaN      NaN   \n",
       "3465          0                                                NaN      NaN   \n",
       "3466          0                                                NaN      NaN   \n",
       "3467          0                                                NaN      NaN   \n",
       "\n",
       "      irony  satire  understatement  overstatement  rhetorical_question  \n",
       "0       1.0     0.0             0.0            0.0                  0.0  \n",
       "1       0.0     0.0             0.0            0.0                  0.0  \n",
       "2       1.0     0.0             0.0            0.0                  0.0  \n",
       "3       0.0     0.0             0.0            0.0                  0.0  \n",
       "4       0.0     0.0             0.0            0.0                  0.0  \n",
       "...     ...     ...             ...            ...                  ...  \n",
       "3463    NaN     NaN             NaN            NaN                  NaN  \n",
       "3464    NaN     NaN             NaN            NaN                  NaN  \n",
       "3465    NaN     NaN             NaN            NaN                  NaN  \n",
       "3466    NaN     NaN             NaN            NaN                  NaN  \n",
       "3467    NaN     NaN             NaN            NaN                  NaN  \n",
       "\n",
       "[3468 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e22cb4e-c043-4afc-8631-2610dd4a0e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3468 entries, 0 to 3467\n",
      "Data columns (total 10 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Unnamed: 0           3468 non-null   int64  \n",
      " 1   tweet                3467 non-null   object \n",
      " 2   sarcastic            3468 non-null   int64  \n",
      " 3   rephrase             867 non-null    object \n",
      " 4   sarcasm              867 non-null    float64\n",
      " 5   irony                867 non-null    float64\n",
      " 6   satire               867 non-null    float64\n",
      " 7   understatement       867 non-null    float64\n",
      " 8   overstatement        867 non-null    float64\n",
      " 9   rhetorical_question  867 non-null    float64\n",
      "dtypes: float64(6), int64(2), object(2)\n",
      "memory usage: 271.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data_en.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec6fdf9-b971-4443-aa27-d495cd32c698",
   "metadata": {},
   "source": [
    "## Remove the data which is NaN，only 1 line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f582553-ce2d-49df-b85f-338efbab06a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_en= data_en.dropna(subset=['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f07f7d04-b618-4c4b-b406-a997d349344c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "      <th>rephrase</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>irony</th>\n",
       "      <th>satire</th>\n",
       "      <th>understatement</th>\n",
       "      <th>overstatement</th>\n",
       "      <th>rhetorical_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The only thing I got from college is a caffein...</td>\n",
       "      <td>1</td>\n",
       "      <td>College is really difficult, expensive, tiring...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I love it when professors draw a big question ...</td>\n",
       "      <td>1</td>\n",
       "      <td>I do not like when professors don’t write out ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Remember the hundred emails from companies whe...</td>\n",
       "      <td>1</td>\n",
       "      <td>I, at the bare minimum, wish companies actuall...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Today my pop-pop told me I was not “forced” to...</td>\n",
       "      <td>1</td>\n",
       "      <td>Today my pop-pop told me I was not \"forced\" to...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
       "      <td>1</td>\n",
       "      <td>I would say Ted Cruz is an asshole and doesn’t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3463</th>\n",
       "      <td>3463</td>\n",
       "      <td>The population spike in Chicago in 9 months is...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>3464</td>\n",
       "      <td>You'd think in the second to last English clas...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>3465</td>\n",
       "      <td>I’m finally surfacing after a holiday to Scotl...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>3466</td>\n",
       "      <td>Couldn't be prouder today. Well done to every ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3467</th>\n",
       "      <td>3467</td>\n",
       "      <td>Overheard as my 13 year old games with a frien...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3467 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              tweet  \\\n",
       "0              0  The only thing I got from college is a caffein...   \n",
       "1              1  I love it when professors draw a big question ...   \n",
       "2              2  Remember the hundred emails from companies whe...   \n",
       "3              3  Today my pop-pop told me I was not “forced” to...   \n",
       "4              4  @VolphanCarol @littlewhitty @mysticalmanatee I...   \n",
       "...          ...                                                ...   \n",
       "3463        3463  The population spike in Chicago in 9 months is...   \n",
       "3464        3464  You'd think in the second to last English clas...   \n",
       "3465        3465  I’m finally surfacing after a holiday to Scotl...   \n",
       "3466        3466  Couldn't be prouder today. Well done to every ...   \n",
       "3467        3467  Overheard as my 13 year old games with a frien...   \n",
       "\n",
       "      sarcastic                                           rephrase  sarcasm  \\\n",
       "0             1  College is really difficult, expensive, tiring...      0.0   \n",
       "1             1  I do not like when professors don’t write out ...      1.0   \n",
       "2             1  I, at the bare minimum, wish companies actuall...      0.0   \n",
       "3             1  Today my pop-pop told me I was not \"forced\" to...      1.0   \n",
       "4             1  I would say Ted Cruz is an asshole and doesn’t...      1.0   \n",
       "...         ...                                                ...      ...   \n",
       "3463          0                                                NaN      NaN   \n",
       "3464          0                                                NaN      NaN   \n",
       "3465          0                                                NaN      NaN   \n",
       "3466          0                                                NaN      NaN   \n",
       "3467          0                                                NaN      NaN   \n",
       "\n",
       "      irony  satire  understatement  overstatement  rhetorical_question  \n",
       "0       1.0     0.0             0.0            0.0                  0.0  \n",
       "1       0.0     0.0             0.0            0.0                  0.0  \n",
       "2       1.0     0.0             0.0            0.0                  0.0  \n",
       "3       0.0     0.0             0.0            0.0                  0.0  \n",
       "4       0.0     0.0             0.0            0.0                  0.0  \n",
       "...     ...     ...             ...            ...                  ...  \n",
       "3463    NaN     NaN             NaN            NaN                  NaN  \n",
       "3464    NaN     NaN             NaN            NaN                  NaN  \n",
       "3465    NaN     NaN             NaN            NaN                  NaN  \n",
       "3466    NaN     NaN             NaN            NaN                  NaN  \n",
       "3467    NaN     NaN             NaN            NaN                  NaN  \n",
       "\n",
       "[3467 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc046ace-20c0-465a-9412-5e439381b5eb",
   "metadata": {},
   "source": [
    "## Word embding wit TF-Idf and use SVM for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9347b226-52b1-4745-8f01-00fba3c26737",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(data_en['tweet'])\n",
    "y = data_en['sarcastic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55b0a5a5-d63d-4e61-b4ef-0b4ec48d2189",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1460806-27fe-4238-9e22-4639105718a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      1.00      0.83       495\n",
      "           1       0.80      0.02      0.04       199\n",
      "\n",
      "    accuracy                           0.72       694\n",
      "   macro avg       0.76      0.51      0.44       694\n",
      "weighted avg       0.74      0.72      0.61       694\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d52121-cdaf-4ebf-bfbf-ef2cc66cdc65",
   "metadata": {},
   "source": [
    "Result of the prediction of label1 is really bad, try other word embding methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84655070-db53-46a0-a556-8dcec4d460c0",
   "metadata": {},
   "source": [
    "# BERT word embding and the result of SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37232565-d6ec-4245-8130-29f3fbc4600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "992eae94-30d6-4f82-a5ab-05ba56cd1a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2fa070-6165-4fd5-bd3e-6cbfd46e234b",
   "metadata": {},
   "source": [
    "texts = data_en['tweet'].tolist()\n",
    "labels = data_en['sarcastic'].tolist()\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d756581b-bfbb-4aca-951a-13a96629c34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data_en['tweet'].tolist()\n",
    "labels = data_en['sarcastic'].tolist()\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    features = outputs.last_hidden_state[:,0,:].numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23a9d42e-a273-4239-bdd7-373cc6e9b2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.82      0.77       495\n",
      "           1       0.36      0.25      0.29       199\n",
      "\n",
      "    accuracy                           0.66       694\n",
      "   macro avg       0.55      0.54      0.53       694\n",
      "weighted avg       0.62      0.66      0.64       694\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "predictions = svm_model.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8ff77f-0a3a-411f-a7cb-7f52c5f7b8c1",
   "metadata": {},
   "source": [
    "The result is much more balanced, the prediction for label1( is sarcasm ) is much better. But time for embding is a little long. We try Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dbf16b-7746-4bbd-9394-e64ed6deabfe",
   "metadata": {},
   "source": [
    "# Word2Vec and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4028215-5f62-4e25-9f48-d7c4cbddbea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f63440d7-0c14-4809-b236-077a26facfb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(866258, 3615700)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model = Word2Vec(sentences=texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "word2vec_model.train(texts, total_examples=len(texts), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48954a35-5dfa-4188-a4f7-dfe2b0bbdc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vector(words, model):\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    else:\n",
    "        return np.mean(vectors, axis=0)\n",
    "sentence_vectors = np.array([get_sentence_vector(text, word2vec_model) for text in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cbe3215-7896-4dc8-8e30-6827a5cc09cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sentence_vectors, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2aeb55c-da7d-434b-a7ca-ad3733ec08df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      1.00      0.83       495\n",
      "           1       0.00      0.00      0.00       199\n",
      "\n",
      "    accuracy                           0.71       694\n",
      "   macro avg       0.36      0.50      0.42       694\n",
      "weighted avg       0.51      0.71      0.59       694\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "predictions = svm_model.predict(X_test)\n",
    "print(classification_report(y_test, predictions, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada2c9aa-d9ed-40ad-a87b-3280ee6739a2",
   "metadata": {},
   "source": [
    "Result is even worse. We use BERT word embding for the other baseline models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c7613d",
   "metadata": {},
   "source": [
    "# BERT embding and CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba47bc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch.optim import Adam\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaForSequenceClassification\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import  get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bd4957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e48ed364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.6256244588981975\n",
      "Epoch 2/30, Loss: 0.6161920482462103\n",
      "Epoch 3/30, Loss: 0.6079820015213706\n",
      "Epoch 4/30, Loss: 0.6001469167796049\n",
      "Epoch 5/30, Loss: 0.5943822345950387\n",
      "Epoch 6/30, Loss: 0.5890877003019507\n",
      "Epoch 7/30, Loss: 0.5863127735528079\n",
      "Epoch 8/30, Loss: 0.5821100581776012\n",
      "Epoch 9/30, Loss: 0.5771567144177177\n",
      "Epoch 10/30, Loss: 0.5738506804813038\n",
      "Epoch 11/30, Loss: 0.5711738033728166\n",
      "Epoch 12/30, Loss: 0.5678739859299227\n",
      "Epoch 13/30, Loss: 0.5678175498138774\n",
      "Epoch 14/30, Loss: 0.5647146295417439\n",
      "Epoch 15/30, Loss: 0.5645117597146467\n",
      "Epoch 16/30, Loss: 0.5633217380805449\n",
      "Epoch 17/30, Loss: 0.5605911179022356\n",
      "Epoch 18/30, Loss: 0.5604793619025837\n",
      "Epoch 19/30, Loss: 0.5606907389380715\n",
      "Epoch 20/30, Loss: 0.5584921552376314\n",
      "Epoch 21/30, Loss: 0.5600046339360151\n",
      "Epoch 22/30, Loss: 0.560171751813455\n",
      "Epoch 23/30, Loss: 0.5577607276764783\n",
      "Epoch 24/30, Loss: 0.5582259879870848\n",
      "Epoch 25/30, Loss: 0.5574367303739894\n",
      "Epoch 26/30, Loss: 0.5577164509079673\n",
      "Epoch 27/30, Loss: 0.5599795498631217\n",
      "Epoch 28/30, Loss: 0.5588255497542295\n",
      "Epoch 29/30, Loss: 0.5565605312585831\n",
      "Epoch 30/30, Loss: 0.5581316609274257\n"
     ]
    }
   ],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 512)  # First fully connected layer\n",
    "        self.relu = nn.ReLU()  # ReLU activation\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout for regularization\n",
    "        self.fc2 = nn.Linear(512, num_classes)  # Final layer for classification\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_features = 768  # For bert-base-uncased\n",
    "num_classes = 2  # Assuming binary classification\n",
    "\n",
    "model = BERTClassifier(num_features=100, num_classes=num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer =  Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc7ac4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      1.00      0.83       495\n",
      "           1       0.00      0.00      0.00       199\n",
      "\n",
      "    accuracy                           0.71       694\n",
      "   macro avg       0.36      0.50      0.42       694\n",
      "weighted avg       0.51      0.71      0.59       694\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model.eval()  \n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "print(classification_report(true_labels, predictions, digits=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a92e25",
   "metadata": {},
   "source": [
    "Performance is horrible. There is reason that people will not use CNN for NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b87119",
   "metadata": {},
   "source": [
    "# BERT embding and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d53d1134",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLSTM(nn.Module):\n",
    "    def __init__(self, bert_model, hidden_dim, output_dim, lstm_layers=1, bidirectional=False, dropout=0.1):\n",
    "        super(BertLSTM, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.lstm = nn.LSTM(768, hidden_dim, num_layers=lstm_layers, bidirectional=bidirectional, batch_first=True, dropout=dropout if lstm_layers > 1 else 0)\n",
    "        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.classifier = nn.Linear(lstm_output_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            sequence_output = outputs.last_hidden_state\n",
    "       \n",
    "        lstm_output, (hidden_state, cell_state) = self.lstm(sequence_output)\n",
    "       \n",
    "        if self.lstm.bidirectional:\n",
    "            lstm_output = torch.cat((hidden_state[-2,:,:], hidden_state[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            lstm_output = hidden_state[-1,:,:]\n",
    "       \n",
    "        logits = self.classifier(lstm_output)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a460cb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "texts = data_en['tweet'].tolist()\n",
    "labels = data_en['sarcastic'].tolist()\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    features = outputs.last_hidden_state[:,0,:].numpy() \n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6596cdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6995932459831238\n",
      "Epoch 2, Loss: 0.61945641040802\n",
      "Epoch 3, Loss: 0.5785393714904785\n",
      "Epoch 4, Loss: 0.576870322227478\n",
      "Epoch 5, Loss: 0.566392719745636\n",
      "Epoch 6, Loss: 0.6753127574920654\n",
      "Epoch 7, Loss: 0.6057244539260864\n",
      "Epoch 8, Loss: 0.5067143440246582\n",
      "Epoch 9, Loss: 0.5833205580711365\n",
      "Epoch 10, Loss: 0.4790968596935272\n",
      "Epoch 11, Loss: 0.553758978843689\n",
      "Epoch 12, Loss: 0.5561097264289856\n",
      "Epoch 13, Loss: 0.5160054564476013\n",
      "Epoch 14, Loss: 0.5935359001159668\n",
      "Epoch 15, Loss: 0.4560028612613678\n",
      "Epoch 16, Loss: 0.48512017726898193\n",
      "Epoch 17, Loss: 0.558098554611206\n",
      "Epoch 18, Loss: 0.5697084665298462\n",
      "Epoch 19, Loss: 0.538184642791748\n",
      "Epoch 20, Loss: 0.5268934369087219\n",
      "Epoch 21, Loss: 0.5719056725502014\n",
      "Epoch 22, Loss: 0.5601086616516113\n",
      "Epoch 23, Loss: 0.5029844641685486\n",
      "Epoch 24, Loss: 0.49454957246780396\n",
      "Epoch 25, Loss: 0.5262494087219238\n",
      "Epoch 26, Loss: 0.5559357404708862\n",
      "Epoch 27, Loss: 0.4932132959365845\n",
      "Epoch 28, Loss: 0.5470383167266846\n",
      "Epoch 29, Loss: 0.5783883333206177\n",
      "Epoch 30, Loss: 0.5989316701889038\n"
     ]
    }
   ],
   "source": [
    "features = torch.tensor(features)\n",
    "labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = BertLSTM(bert_model=model, hidden_dim=256, output_dim=2, lstm_layers=1, bidirectional=True).to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 30\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94776399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      1.00      0.85       511\n",
      "           1       0.00      0.00      0.00       183\n",
      "\n",
      "    accuracy                           0.74       694\n",
      "   macro avg       0.37      0.50      0.42       694\n",
      "weighted avg       0.54      0.74      0.62       694\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        pred_labels.extend(predictions.cpu().numpy())\n",
    "\n",
    "\n",
    "print(classification_report(true_labels, pred_labels, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970427e5",
   "metadata": {},
   "source": [
    "LSTM is not a good idea neither.                                                                  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109e1e35",
   "metadata": {},
   "source": [
    "# BERT embding and RoBERTa(PTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a73e4fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2) # Assuming binary classification (e.g., sarcastic vs. not sarcastic)\n",
    "\n",
    "\n",
    "texts = data_en['tweet'].tolist()\n",
    "labels = data_en['sarcastic'].tolist()\n",
    "\n",
    "\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
    "validation_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41370f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_24504\\1129277803.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 30 ========\n",
      "  Batch     0  of     22.\n",
      "  Average training loss: 0.60\n",
      "  Validation Accuracy: 0.76\n",
      "======== Epoch 2 / 30 ========\n",
      "  Batch     0  of     22.\n",
      "  Average training loss: 0.57\n",
      "  Validation Accuracy: 0.76\n",
      "======== Epoch 3 / 30 ========\n",
      "  Batch     0  of     22.\n",
      "  Average training loss: 0.55\n",
      "  Validation Accuracy: 0.76\n",
      "======== Epoch 4 / 30 ========\n",
      "  Batch     0  of     22.\n",
      "  Average training loss: 0.54\n",
      "  Validation Accuracy: 0.76\n",
      "======== Epoch 5 / 30 ========\n",
      "  Batch     0  of     22.\n",
      "  Average training loss: 0.50\n",
      "  Validation Accuracy: 0.76\n",
      "======== Epoch 6 / 30 ========\n",
      "  Batch     0  of     22.\n",
      "  Average training loss: 0.45\n",
      "  Validation Accuracy: 0.75\n",
      "======== Epoch 7 / 30 ========\n",
      "  Batch     0  of     22.\n",
      "  Average training loss: 0.41\n",
      "  Validation Accuracy: 0.73\n",
      "======== Epoch 8 / 30 ========\n",
      "  Batch     0  of     22.\n",
      "  Average training loss: 0.39\n",
      "  Validation Accuracy: 0.74\n",
      "======== Epoch 9 / 30 ========\n",
      "  Batch     0  of     22.\n",
      "  Average training loss: 0.33\n",
      "  Validation Accuracy: 0.74\n",
      "======== Epoch 10 / 30 ========\n",
      "  Batch     0  of     22.\n",
      "  Average training loss: 0.28\n",
      "  Validation Accuracy: 0.73\n",
      "======== Epoch 11 / 30 ========\n",
      "  Batch     0  of     22.\n",
      "  Average training loss: 0.24\n",
      "  Validation Accuracy: 0.74\n",
      "======== Epoch 12 / 30 ========\n",
      "  Batch     0  of     22.\n",
      "  Average training loss: 0.20\n",
      "  Validation Accuracy: 0.74\n",
      "======== Epoch 13 / 30 ========\n",
      "  Batch     0  of     22.\n",
      "  Average training loss: 0.17\n",
      "  Validation Accuracy: 0.72\n",
      "======== Epoch 14 / 30 ========\n",
      "  Batch     0  of     22.\n",
      "  Average training loss: 0.14\n",
      "  Validation Accuracy: 0.73\n",
      "======== Epoch 15 / 30 ========\n",
      "  Batch     0  of     22.\n",
      "  Average training loss: 0.11\n",
      "  Validation Accuracy: 0.74\n",
      "======== Epoch 16 / 30 ========\n",
      "  Batch     0  of     22.\n",
      "  Average training loss: 0.10\n",
      "  Validation Accuracy: 0.73\n",
      "======== Epoch 17 / 30 ========\n",
      "  Batch     0  of     22.\n",
      "  Average training loss: 0.09\n",
      "  Validation Accuracy: 0.73\n",
      "======== Epoch 18 / 30 ========\n",
      "  Batch     0  of     22.\n",
      "  Average training loss: 0.09\n",
      "  Validation Accuracy: 0.73\n",
      "======== Epoch 19 / 30 ========\n",
      "  Batch     0  of     22.\n",
      "  Average training loss: 0.06\n",
      "  Validation Accuracy: 0.74\n",
      "======== Epoch 20 / 30 ========\n",
      "  Batch     0  of     22.\n",
      "  Average training loss: 0.07\n",
      "  Validation Accuracy: 0.72\n",
      "======== Epoch 21 / 30 ========\n",
      "  Batch     0  of     22.\n",
      "  Average training loss: 0.06\n",
      "  Validation Accuracy: 0.74\n",
      "======== Epoch 22 / 30 ========\n",
      "  Batch     0  of     22.\n",
      "  Average training loss: 0.06\n",
      "  Validation Accuracy: 0.73\n",
      "======== Epoch 23 / 30 ========\n",
      "  Batch     0  of     22.\n",
      "  Average training loss: 0.05\n",
      "  Validation Accuracy: 0.74\n",
      "======== Epoch 24 / 30 ========\n",
      "  Batch     0  of     22.\n",
      "  Average training loss: 0.04\n",
      "  Validation Accuracy: 0.74\n",
      "======== Epoch 25 / 30 ========\n",
      "  Batch     0  of     22.\n",
      "  Average training loss: 0.03\n",
      "  Validation Accuracy: 0.74\n",
      "======== Epoch 26 / 30 ========\n",
      "  Batch     0  of     22.\n",
      "  Average training loss: 0.05\n",
      "  Validation Accuracy: 0.74\n",
      "======== Epoch 27 / 30 ========\n",
      "  Batch     0  of     22.\n",
      "  Average training loss: 0.03\n",
      "  Validation Accuracy: 0.73\n",
      "======== Epoch 28 / 30 ========\n",
      "  Batch     0  of     22.\n",
      "  Average training loss: 0.03\n",
      "  Validation Accuracy: 0.74\n",
      "======== Epoch 29 / 30 ========\n",
      "  Batch     0  of     22.\n",
      "  Average training loss: 0.03\n",
      "  Validation Accuracy: 0.74\n",
      "======== Epoch 30 / 30 ========\n",
      "  Batch     0  of     22.\n",
      "  Average training loss: 0.03\n",
      "  Validation Accuracy: 0.74\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "input_ids = inputs['input_ids']\n",
    "attention_masks = inputs['attention_mask']\n",
    "\n",
    "\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n",
    "validation_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)\n",
    "\n",
    "\n",
    "optimizer =  Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "epochs = 30\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Helper Function for Accuracy Calculation\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# Training Loop\n",
    "for epoch_i in range(0, epochs):\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 40 == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "        \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "\n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    eval_accuracy = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"  Validation Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05f340e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.88      0.83       520\n",
      "           1       0.47      0.32      0.38       174\n",
      "\n",
      "    accuracy                           0.74       694\n",
      "   macro avg       0.63      0.60      0.61       694\n",
      "weighted avg       0.71      0.74      0.72       694\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "model.eval()\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in validation_dataloader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Convert model logits to class probabilities using softmax\n",
    "        probs = softmax(outputs.logits, dim=1)\n",
    "        \n",
    "        # Get the predicted labels\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "        \n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        pred_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "report = classification_report(true_labels, pred_labels, zero_division=0)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0e6ed7",
   "metadata": {},
   "source": [
    "More fine-tuning may needed. Much better than LSTM in variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2257d4",
   "metadata": {},
   "source": [
    "# Conlusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ea1f6b",
   "metadata": {},
   "source": [
    "1.We have built 4 baseline models, which can give us an idea about the futhur work.                                  \n",
    "2.We will try to reproduce the results of SemEval-2022 Task 6: iSarcasmEval, Intended Sarcasm Detection in\n",
    "English and Arabic.                                                                              \n",
    "3.We will use data augmentation and more word embding methods trying to have a better result.                                                       \n",
    "4.XLM-RoBERTa, LIama2, and XGen will also be tried. We will read more papers to have an idea about fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d58b05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
